
[15] Yuan He, Lirui Liu, Jing Liu, Weijia Wu, Hao Zhou, and Bohan Zhuang. Zipcache: Accurate and efficient kv cache quantization for llms. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [16] Yinnin Zhong et al. Distserve: Disaggregating prefill and decoding for llm serving. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2024. [17] Jianchun Liu, Jiaming Yan, Hongli Xu, Zhiyuan Wang, Jinyang Huang, and Yang Xu. Finch: Enhancing federated learning with hierarchical neural architecture search. IEEE Transactions on Mobile Computing, 23(5):6012- 6026, 2024. [18] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin. Ragcache: Efficient knowledge caching for retrieval- augmented generation. arXiv preprint arXiv:2404.12457, 2024. [19] Xinqi Chu, Jingdong Chen, Jun Yin, Chuang Niu, Zhuliang Yao, Yilei Huang, Tianbao Li, and et al. Mobilevlm v2: Faster and stronger baseline for vision language model on mobile devices. arXiv preprint arXiv:2402.03766, 2024. [20] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [21] Xiang Zhang, Yan Lu, Huan Yan, Jinyang Huang, Yu Gu, Yusheng Ji, Zhi Liu, and Bin Liu. Resup: Reliable label noise suppression for facial expression recognition. IEEE Transactions on Affective Computing, pages 1- 14, 2025. [22] Mingjin Zhang, Xiaoming Shen, Jiannong Cao, Zeyang Cui, and Shihan Jiang. Edgeshard: Efficient llm inference via collaborative edge computing. arXiv preprint arXiv:2405.14371, 2024. [23] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming throughput- latency tradeoff in llm inference with sarathi- serve. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI),2024. [24] Lee et al. Infinigen: Efficient llm inference via dynamically managed kv cache. In USENIX OSDI, 2024. [25] Yu Gu, Huan Yan, Xiang Zhang, Yantong Wang, Jinyang Huang, Yusheng Ji, and Fuji Ren. Attention- based gesture recognition using commodity wifi devices. IEEE Sensors Journal, 23(9):9685- 9696, 2023. [26] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine- tuning of large models. In Proceedings of the 61st Annual Meeting of the ACL (System Demonstrations), pages 558- 568, 2023. [27] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hongyu Wu, Jeff Demouth, and Song Han. Smoothquant: Accurate and efficient post- training quantization for large language models. In International Conference on Machine Learning (ICML), 2023. [28] Jianbo Wu, Jie Ren, Shuangyan Yang, Konstantinos Parasyris, Giorgis Georgakoudis, Ignacio Laguna, and Dong Li. Lm- offload: Performance model- guided generative inference of large language models with parallelism control. Technical Report, 2024. [29] Jinyang Huang, Bin Liu, Chao Chen, Hongxin Jin, Zhiqiang Liu, Chi Zhang, and Nenghai Yu. Towards anti- interference human activity recognition based on wifi subcarrier correlation selection. IEEE Transactions on Vehicular Technology, 69(6):6739- 6754, 2020. [30] Jiajun Xu, Zhiyuan Li, Wei Chen, Qun Wang, Xin Gao, Qi Cai, and Ziyuan Ling. On- device language models: A comprehensive review. arXiv preprint arXiv:2409.00088, 2024. [31] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specifier: Accelerating generative large language model serving with tree- based speculative inference and verification. arXiv preprint arXiv:2305.09781, 2024. [32] Peng Zhao, Jinyang Huang, Xiang Zhang, Zhi Liu, Huan Yan, Meng Wang, Guohang Zhuang, Yutong Guo, Xiao Sun, and Meng Li. Wipulmo: Commodity wifi can capture your pulmonary function without mouth clinging. IEEE Internet of Things Journal, 12(1):854- 868, 2025. [33] Korakit Seemakhupt, Sihang Liu, and Samira Khan. Edgerag: Online- indexed rag for edge devices. arXiv preprint arXiv:2412.21023, 2024. [34] Yu Gu, Xiang Zhang, Huan Yan, Jinyang Huang, Zhi Liu, Mianxiong Dong, and Fuji Ren. Wife: Wifi and vision based unobtrusive emotion recognition via gesture and facial expression. IEEE Transactions on Affective Computing, 14(4):2567- 2581, 2023.  


[35] Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, S. Karen Khatamifard, Minsik Cho, Carlo C. Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. Llm in a flash: Efficient large language model inference with limited memory. arXiv preprint arXiv:2312.11514, 2024. [36] Zhenyu Zhang et al. H2o: Heavy- hitter oracle for efficient generative inference of large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [37] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning- free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [38] Jinyang Huang, Jia- Xuan Bai, Xiang Zhang, Zhi Liu, Yuanhao Feng, Jianchun Liu, Xiao Sun, Mianxiong Dong, and Meng Li. Keystrokesniffer: An off- the- shelf smartphone can eavesdrop on your privacy from anywhere. IEEE Transactions on Information Forensics and Security, 19:6840- 6855, 2024. [39] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia- Bin Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and generating speech, music, sound, and talking head. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [40] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. [41] Hanshi Sun, Li- Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, and Beidi Chen. Shadowkv: Kv cache in shadows for high- throughput long- context llm inference. arXiv preprint arXiv:2410.21465, 2024. [42] Jinyang Huang, Bin Liu, Chenglin Miao, Xiang Zhang, Jianchun Liu, Lu Su, Zhi Liu, and Yu Gu. Phyfinatt: An undetectable attack framework against phy layer fingerprint- based wifi authentication. IEEE Transactions on Mobile Computing, 23(7):7753- 7770, 2024. [43] Hongpeng Jin and Yanzhao Wu. Ce- collm: System design for efficient cloud- edge collaborative large language models. arXiv preprint arXiv:2411.02829, 2024. [44] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2024. [45] Xiang Zhang, Jinyang Huang, Huan Yan, Yuanhao Feng, Peng Zhao, Guohang Zhuang, Zhi Liu, and Bin Liu. Wipon: A robust wi- fi- based open- set gesture recognition framework. IEEE Transactions on Human- Machine Systems, 55(2):234- 245, 2025. [46] Yue Zheng, Yuhao Chen, Bin Qian, Xiufang Shi, Yuanchao Shu, and Jiming Chen. A review on edge large language models: Design, execution, and applications. ACM Computing Surveys, 2025. [47] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. arXiv preprint arXiv:2309.06180, 2023. [48] Yuanhao Feng, Jinyang Huang, Youwei Zhang, Xiang Zhang, Meng Li, Fusang Zhang, Tianyue Zheng, Anran Li, Mianxiong Dong, and Zhi Liu. Rf- eye: Commodity rfid can know what you write and who you are wherever you are. ACM Transactions on Sensor Networks, 2025. [49] Zonghan Yang, Yu Yu, Qianlin Wang, Weibin Meng, Peng Cheng, Hongke Zhou, and Siheng Chen. Customizing llms for efficient latency- aware inference at the edge. In USENIX Annual Technical Conference (ATC), 2025. [50] Wang Ye et al. Instinfer: In- storage attention offloading for cost- effective llm inference. arXiv preprint arXiv:2409.04992, 2024. [51] Jianchun Liu, Hongli Xu, Lun Wang, Yang Xu, Chen Qian, Jinyang Huang, and He Huang. Adaptive asynchronous federated learning in resource- constrained edge computing. IEEE Transactions on Mobile Computing, 22(2):674- 690, 2021. [52] Liang Chen et al. Punica: Multi- tenant lora serving. In Proceedings of MLSys, 2024. [53] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. In Proceedings of the 41st International Conference on Machine Learning, PMLR, 2024. [54] Christopher Holmes, Sefa Tanaka, et al. Deepspeed- fastgen: High- throughput text generation for llms via mii and deepspeed- inference. arXiv preprint arXiv:2401.08671, 2024.