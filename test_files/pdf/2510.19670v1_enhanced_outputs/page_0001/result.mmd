
# CoSense-LLM: Semantics-at-the-Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation  


Hasan Akgul Mari Eplik Department of Computer Engineering Institute of Computer Science Istanbul Technical University (ITU) University of Tartu ITU Ayazaga Campus, 34469, Maslak/Istanbul, Turkey J. Liivi 2, 50409 Tartu, Estonia akgulh20@itu.edu.tr mari.eplik@ut.ee Javier Rojas Aina Binti Abdullah Pieter van der Merwe Department of Computer Science Faculty of Computing Dept. of Electrical & Electronic Engineering University of Chile Universiti Teknologi Malaysia Stellenbosch University Beauchef 851, Santiago, Chile 81310 Skudai, Johor, Malaysia Banghoek Rd, Stellenbosch, 7600, South Africa jrojas@dcc.uchile.cl aina.abdullah@utm.my pvdm@sun.ac.za  


Abstract—We present CoSense- LLM, an edge- first framework that converts continuous multimodal sensor streams (e.g., Wi- Fi CSI, IMU, audio, RFID, lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models (LLMs) under explicit latency, energy, bandwidth, and privacy constraints. CoSense- LLM comprises four components: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short, discrete code sequences; (ii) Edge- RAG, a local hybrid retrieval layer that grounds generation in site- specific policies and notes; (iii) PromptRouter, a cost- and uncertainty- aware policy that selects among edge- only generation, edge- retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system is compatible with modern serving optimizations—including paged/streaming KV caches, FlashAttention- style kernels, speculative decoding, and quantized LoRA adapters—and supports on- device personalization and federated updates under non- IID drift. Across home, office, and clinic deployments with day- long streams, CoSense- LLM delivers grounded explanations while meeting tight service- level objectives: it sustains sub- second (p95) end- to- end latency on edge- dominant paths, reduces inter- tier token and bandwidth costs by preferring local, retrieval- grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge- RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV/decoding accelerators materially lower energy per decision. The results support an edge- first design that treats semantics, privacy, and predictable latency as co- equal goals for large- model deployments in interference- prone environments.  


Index Terms—Edge intelligence, large language models, cloud- edge collaboration, retrieval- augmented generation, multimodal sensing, Wi- Fi CSI, RFID, uncertainty- aware routing, quantization, KV cache, speculative decoding, LoRA/QLoRA, federated learning, privacy and auditability  


## I. INTRODUCTION  


Edge intelligence is moving from a vision of offloading a few convolutional layers to a concrete capability of running, splitting, and personalizing large language and multimodal models close to where data is produced. Recent surveys and system papers converge on two forces behind this shift: (i) the need to reduce end- to- end latency and bandwidth while respecting privacy; and (ii) the emergence of algorithm- system co- design that compresses models, manages memory, and orchestrates collaborative execution across device, edge, and cloud [19], [30], [46], [58]. Cloud- edge collaboration for LLMs is now studied not only at the scheduler level but also at the model- graph level, enabling partitioned or cooperative inference and training [22], [26], [43], [70]. Meanwhile, pervasive sensing applications—e.g., Wi- Fi CSI and RFID based activity or gesture understanding—motivate low- leakage, low- latency interpretation pipelines at the network edge rather than sending raw signals to distant servers [9], [29].   


A central challenge in edge LLM serving is memory and state management under tight resources. Modern serving stacks rely on paged key- value (KV) caches, dynamic cache admission/eviction, and streaming to decouple prefill and decode without stalling the device [7], [12], [15], [24], [37], [47], [71]. Kernel- level optimizations such as FlashAttention- 2 and GPU- side decoding pipelines further increase throughput [6], [11]. To keep conversational quality under constrained context windows, streaming designs maintain “attention sinks” and consistent states across long sessions [44]. System- level disaggregation and scheduling control prefill—decode tradeoffs and cluster utilization [16], [23], [54], while shadow and hierarchical caches improve long- context efficiency [41]. These techniques are complementary to robustness- oriented sensing pipelines that must survive interference or distribution drifts before any language reasoning happens [5], [13].  


Offloading and storage- aware execution provide another axis of feasibility for edge scenarios. Single- GPU and heterogeneous CPU/NVMe pipelines amortize model state movement [14], [35], and performance- model- guided offloaders decide tensor placement and parallelism under device limits [28]. In- storage attention and serverless runtimes reduce hot- standby costs while keeping tail latency controlled for bursty workloads at the network edge [8], [50]. Beyond placement,