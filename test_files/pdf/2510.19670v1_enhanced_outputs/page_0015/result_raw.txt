<|ref|>image<|/ref|><|det|>[[101, 65, 465, 272]]<|/det|>
<|ref|>image_caption<|/ref|><|det|>[[75, 281, 489, 311]]<|/det|>
<center>Fig. 5: ROC for unknown-event detection (TS protocol): CoSense-LLM vs. edge classifier baseline. </center>  

<|ref|>text<|/ref|><|det|>[[78, 341, 489, 416]]<|/det|>
over- abstains on long, safe routines; too much on the draft head ignores upstream sensing ambiguity. A middle ground yields the best AURC and SLA hit rate. The observed monotonicity resembles calibration effects reported for streaming LLMs [44].  

<|ref|>text<|/ref|><|det|>[[78, 418, 489, 508]]<|/det|>
b) Thresholds \(\theta\) : Higher \(\theta_{\mathrm{edge}}\) shifts mass from EDGE-ONLY to EDGE+RAG and ESCALATE, improving factual consistency but increasing energy and latency. We choose \(\theta\) via regret minimization on validation streams as described in Method; the resulting operating point achieves robust selective accuracy without exhausting token budgets.  

<|ref|>text<|/ref|><|det|>[[108, 510, 325, 524]]<|/det|>
3) Quantization, KV, and Kernels:  

<|ref|>text<|/ref|><|det|>[[78, 526, 489, 646]]<|/det|>
a) Weight precision.: INT8 and 4-bit quantization incur small quality drops (macro-F1, BERTScore) relative to FP16 but significantly reduce energy and latency. The tradeoff is dominated by decoder matmuls and memory movement, so kernel improvements like FlashAttention-2 and fused ops amplify gains [11]. These trends align with activation-aware and system co-design quantization literature [4], [10] and post-training schemes [2], [27].  

<|ref|>text<|/ref|><|det|>[[78, 648, 489, 753]]<|/det|>
b) KV compression.: ZipCache-like 4-bit and KIVI-like 2-bit KV quantization reduce resident memory and improve p95 latency on long sessions; selective outlier handling avoids major quality regressions [15], [37]. Context streaming further trims prefill cost for large prompts [12]. In ESCALATE, remote prefill dominates; we therefore cap prompt length and exploit retrieval summaries.  

<|ref|>text<|/ref|><|det|>[[78, 755, 489, 829]]<|/det|>
c) Paged vs. streaming KV.: Paged KV handles concurrency well, while streaming shines on extended contexts with incremental growth [47], [71]. We prefer paged KV for short answers; for long policy explanations we switch to streaming, improving FTT stability under deep histories.  

<|ref|>text<|/ref|><|det|>[[78, 831, 489, 905]]<|/det|>
4) Decoding Accelerators: Enabling Medusa and lookahead speeds up token emission for EDGE-ONLY/EDGE+RAG by reducing the number of committed forward steps per token [40], [53]. Gains are largest when the draft head is well-aligned with local prompts, echoing speculative verification designs  

<|ref|>image<|/ref|><|det|>[[531, 64, 891, 250]]<|/det|>
<|ref|>image_caption<|/ref|><|det|>[[506, 260, 919, 290]]<|/det|>
<center>Fig. 6: Riskâ€“coverage curves on HL. Lower risk at the same coverage indicates better selective decisions. </center>  

<|ref|>text<|/ref|><|det|>[[506, 319, 918, 363]]<|/det|>
[31], [72]. The router accounts for accelerator availability in its cost predictors and tilts toward local generation when accelerators are active.  

<|ref|>text<|/ref|><|det|>[[523, 365, 816, 379]]<|/det|>
5) Adapters and Federated Personalization:  

<|ref|>text<|/ref|><|det|>[[507, 380, 918, 440]]<|/det|>
a) Local LoRA rank.: LoRA ranks \(\{4,8,16\}\) show diminishing returns; higher ranks marginally improve CE but cost memory and energy. INT4+LoRA yields the best Pareto front for on-edge specialization [20], [74].  

<|ref|>text<|/ref|><|det|>[[507, 441, 918, 666]]<|/det|>
b) Federated rounds.: Periodic aggregation across sites improves CE and TS by countering site-specific biases, with modest communication overhead thanks to sparsified codebook updates and LoRA deltas. Catastrophic forgetting is mitigated by elastic penalties and, when needed, block expansion with later pruning, consistent with recent federated fine-tuning strategies [62], [68]. Experience-driven model migration at the edge [5] and hierarchical aggregation [56] are complementary; our results indicate that light-weight adapter exchange is sufficient for the studied scales. Personalized FL frameworks (Ferrari, Peaches) advocate per-client specialization and NAS for heterogeneous clients; we see similar benefits at small memory cost [60], [61]. Our findings also harmonize with federated NAS for sensing encoders [17] and communication-efficient async updates [51], [55].  

<|ref|>sub_title<|/ref|><|det|>[[508, 675, 738, 689]]<|/det|>
### D. Visualization and Case Studies  

<|ref|>text<|/ref|><|det|>[[507, 695, 918, 724]]<|/det|>
Although we do not display figures here, we summarize qualitative insights tied to Fig. 1 and the metrics.  

<|ref|>text<|/ref|><|det|>[[507, 725, 918, 860]]<|/det|>
a) Code-space structure.: t-SNE/UMAP of VQ codes reveals modality-specific axes (RF-attenuation patterns vs. IMU rhythms) that are progressively entangled through fusion. Under CE, new furniture changes shift clusters modestly; after federated rounds, codes realign, consistent with personalization effects noted in Wi-Fi activity and open-set gesture recognition [25], [45]. Ablating Edge-RAG increases the entropy of explanation tokens; adding retrieval tightens the distribution around policy-anchored phrases.  

<|ref|>text<|/ref|><|det|>[[507, 861, 918, 905]]<|/det|>
b) Event timelines and retrieval attributions.: Per-room timelines show that EDGE-ONLY dominates for routine episodes, with EDGE+RAG spikes around anomalies (door