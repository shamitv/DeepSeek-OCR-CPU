<|ref|>image<|/ref|><|det|>[[100, 73, 456, 245]]<|/det|>
<|ref|>image_caption<|/ref|><|det|>[[77, 255, 489, 286]]<|/det|>
<center>Fig. 7: Ablation on retrieval depth \(k\) : factual consistency (left axis) vs. latency (right axis) on LO. </center>  

<|ref|>image<|/ref|><|det|>[[99, 305, 456, 500]]<|/det|>
<|ref|>image_caption<|/ref|><|det|>[[77, 509, 489, 539]]<|/det|>
<center>Fig. 8: Throughput vs. p95 latency under the Moderate link across methods. </center>  

<|ref|>text<|/ref|><|det|>[[79, 568, 489, 643]]<|/det|>
linger, cart motion in restricted zones). Sentence- level citations to retrieved snippets appear in \(70 - 80\%\) of outputs; contradiction checks via the on- edge NLI head flag a small residual set, which the router escalates or abstains from. This matches the expected workflow of hybrid retrieval [33].  

<|ref|>text<|/ref|><|det|>[[79, 645, 489, 796]]<|/det|>
c) Failure modes.: Key failures include: (i) subtle multipath changes causing code misassignment; (ii) over- summarization when the router forces short outputs under energy pressure; and (iii) misleading retrievals when sparse matches dominate. The first is mitigated by adapter updates and federated rounds; the second by learned length control; the third by raising \(\Gamma\) weight and preferring dense+consistency-checked bundles, akin to RAGCache heuristics [18]. In HM, rare paired cough and posture changes occasionally trigger escalations; we accept the latency hit for safety.  

<|ref|>sub_title<|/ref|><|det|>[[79, 811, 261, 825]]<|/det|>
### E. Robustness and Privacy  

<|ref|>text<|/ref|><|det|>[[79, 832, 489, 906]]<|/det|>
a) Interference and drift.: Under induced interference (microwave on, additional reflective surfaces), SenseFusion's phase- and correlation-robust processing holds up better than amplitude-only features, echoing PhaseAnti and correlation-selection results [13], [29]. Macro-F1 drops modestly but  

<|ref|>text<|/ref|><|det|>[[507, 62, 917, 92]]<|/det|>
recovers after light adapter updates. Open- set abstention rises appropriately; escalations increase by design.  

<|ref|>text<|/ref|><|det|>[[508, 94, 918, 229]]<|/det|>
b) Security probes.: We emulate PHY fingerprint attacks and acoustic side-channels by injecting edge-node probes. Because raw waveforms never leave the device and prompts carry only codes and redacted metadata, WAN exposure of sensitive patterns is negligible. PII leakage risk remains near-zero after redaction; audit completeness is high as the router logs state and redactions per decision. These practices respond directly to demonstrated attack vectors in PHY authentication and acoustic eavesdropping [38], [42].  

<|ref|>text<|/ref|><|det|>[[508, 231, 918, 336]]<|/det|>
c) Privacy vs. utility.: We quantify privacy proxies (zero raw leakage and near-zero PII strings) alongside utility; grounded explanations retain clinical and operational value without transmitting raw data. This supports site adoption in scenarios where camera-based monitoring is unacceptable and RF/IMU alternatives are preferred, in line with Wi-Fi-based emotion and respiratory sensing works [9], [32], [34].  

<|ref|>sub_title<|/ref|><|det|>[[508, 355, 680, 369]]<|/det|>
### F. Sensitivity and Scaling  

<|ref|>text<|/ref|><|det|>[[507, 377, 918, 452]]<|/det|>
a) Code length \(K\) : Increasing \(K\) improves recognition and explanation consistency until saturation; beyond 32, latency and tokens rise with little gain. We adopt \(K = 16\) in most runs. This confirms the value of compact semantic interfaces for edge-cloud cooperation.  

<|ref|>text<|/ref|><|det|>[[508, 455, 918, 545]]<|/det|>
b) Retrieval parameters.: Dense embedding dimension (256-768) trades off recall and latency; HNSW parameters \((M,ef)\) modulate tail latency. We find 384-d embeddings with \(M = 32\) and \(ef = 64\) adequate; raising \(ef\) further trims contradiction at a cost in tail latency, echoing hybrid retrieval tuning advice [33].  

<|ref|>text<|/ref|><|det|>[[508, 546, 918, 636]]<|/det|>
c) WAN profiles and serverless colds.: In Poor WAN, escalation p95 grows with cold starts in serverless baselines [8]. The router curtails escalations and caps output length, sustaining SLA hit rate. Partitioned serving remains sensitive to WAN jitter because each request traverses the link; our approach localizes routine decisions and attributions.  

<|ref|>text<|/ref|><|det|>[[508, 639, 918, 744]]<|/det|>
d) Concurrency and memory headroom.: Streaming KV and ZipCache-like quantization allow more concurrent sessions before thrashing [12], [15]. The cache hit ratio grows with temporal locality; EDGE+RAG reuses prior doc IDs across adjacent windows. Compared to vLLM-style paging alone [47], combining paging with streaming for long histories yields more stable FTT.  

<|ref|>sub_title<|/ref|><|det|>[[508, 763, 750, 777]]<|/det|>
### G. Comparisons to Related Systems  

<|ref|>text<|/ref|><|det|>[[508, 786, 918, 906]]<|/det|>
a) Split and collaborative serving.: Compared to cloud-edge split stacks [22], [43], [70], our pipeline reduces WAN tokens and avoids streaming raw or verbose sensory descriptions, cutting latency and privacy risk on routine tasks. Collaborative designs like PETALS hint at peer-to-peer sharing for model shards [26]; our caching and attribution mechanisms would extend naturally to such settings, with audit trails necessary for multi-tenant edges.