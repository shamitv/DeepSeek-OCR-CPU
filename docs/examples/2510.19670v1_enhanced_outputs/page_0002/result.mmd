
inference- time acceleration leverages program- style LM execution [1], speculative decoding with verification or multi- head drafting [31], [40], [53], [72], token- utility- aware strategies [36], [73], and kernel- level attention improvements already noted [11]. These advances complement asynchronous and communication- efficient learning frameworks that adapt to edge heterogeneity during training [51], [55].  


Personalization and compression are equally decisive at the edge. Low- rank adaptation and its quantized fine- tuning variant enable per- tenant specialization within tight memory budgets [20], [74]. Post- training quantization and activation- aware schemes, combined with serving- time co- design, bring 4- 8- bit regimes into production without prohibitive quality loss [2], [4], [10]. Multi- tenant adapter serving avoids coldstart and context bloat by dynamically selecting adapters for tenants and tasks [3], [52]. In parallel, federated and semisupervised edge learning methods tailor encoders and adapters for user or site- specific distributions [17], [60], [61], [75]- [77]. Such ingredients are crucial when the edge must understand noisy signals or site- specific semantics before handing off compact evidence to an LLM.  


Retrieval- augmented generation (RAG) at the edge addresses knowledge staleness and privacy by grounding responses in local corpora and telemetry. Online indexing and cacheable knowledge chunks improve hit rates under intermittent connectivity [18], [33]. Hardware- algorithm codesign shows how computing- in- memory can robustly realize RAG primitives close to sensors [78], while collaborative or decentralized serving amortizes expensive backbone layers across peers [26]. Bridging continuous signals and language further benefits from instruction- tuned multimodal models that map non- textual inputs to textual interfaces [39], [57]. This is especially relevant for Wi- Fi/IMU/RFID/vision scenarios such as attention- based gesture recognition, unobtrusive emotion inference, pulmonary function sensing, open- set gestures, and affective modeling under label noise [21], [25], [32], [34], [45], as well as RFID- based writing and identity sensing in the wild [48].  


Security and privacy considerations sharpen the case for edge- first pipelines. Physical- layer authentication can be subverted via fingerprint- level attacks, motivating local verification, data minimization, and verifiable reasoning where possible [42]. Side- channel risks such as acoustic keystroke eavesdropping also argue for on- device processing and careful prompt/trace handling to limit raw data exposure [38]. In response, modern edge intelligence stacks pair robust signal encoders with privacy- preserving learning and adapter isolation, and exploit streaming or shard- based execution to avoid shipping sensitive context to the cloud [14], [22], [43], [70].  


Against this backdrop, our work targets low- latency, privacy- preserving event understanding that converts raw, noisy multimodal signals into compact semantic evidence at the edge, and coordinates with large models only when necessary. We build on scheduling and cache- management advances [7], [12], [15], [24], [37], [47], [71], offloading and kernel pipelines [6], [11], [14], [16], [23], [28], [35], [54],  


and decoding accelerators [31], [36], [40], [53], [72], [73], while weaving in federated and decentralized optimization to cope with dynamic, non- IID edge populations [56], [63], [64], [66]. Our adapter pathway embraces quantized and LoRA- style fine- tuning for personalization [2], [4], [10], [20], [74], with federated variants to mitigate forgetting and communication load [62], [68]. For sampling and pipeline stability, we employ robust token selection and near bubble- free end- cloud scheduling [67], [69]. The result is a cloud- edge collaboration pattern that treats local encoders and RAG stores as first- class citizens, exploits collaborative or peer- based sharing when safe [26], and targets the interference- prone sensing settings seen in practice [9], [13].  


Finally, we articulate three practical objectives that guide the design and evaluation in the rest of the paper. First, semantics- at- the- edge: turn sensor windows into compact, verifiable semantic tokens before any cross- boundary transmission, building on multimodal alignment [39], [57]. Second, resource- aware cooperation: decide when to stop locally, when to request retrieval, and when to escalate across tiers using cost and uncertainty signals informed by the serving literature [16], [22], [23], [43], [70]. Third, site- specific learning: personalize with quantized adapters and federated optimization for the distributions embodied by our Wi- Fi/RFID/vision use cases [17], [21], [25], [32], [34], [45], [48], [60], [61], [75]- [77]. These choices reflect both the opportunities and constraints of deploying LLM- era intelligence within the edge computing envelope.  


## II. RELATED WORK  


This section reviews the landscape of large- model execution in edge settings, spanning collaborative partitioning and scheduling, memory and state management for long- context serving, inference- time acceleration, compression and adapter serving for personalization, serverless and storage- aware backends, retrieval- augmented generation (RAG) at the edge, multimodal signal- to- language bridges close to sensors, and federated optimization for non- IID populations. We interleave system advances with sensing- driven applications to underline why edge- first design is not merely an optimization choice but often a functional requirement in interference- prone, privacy- critical environments.  


### A. Cloud-Edge Collaboration and Partitioned LLM Serving  


Pioneering efforts in collaborative inference aim to split or shard the forward graph across device, edge, and cloud in ways that track resource envelopes and network variability. EdgeShard formulates collaborative LLM inference as an edge- centric problem, sharding computation to minimize round trips and balance compute under bandwidth constraints [22]. SplitLLM generalizes split inference with policies that decide cut points and synchronize hidden states efficiently across heterogeneous nodes, demonstrating sizable latency wins under practical links [70]. CE- CoLLM broadens the design space to system- level abstractions, arguing for end- to- end models of cost that include tokenization, prefill, decoding,