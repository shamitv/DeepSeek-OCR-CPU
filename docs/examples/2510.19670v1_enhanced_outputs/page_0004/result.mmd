
on a single GPU using heterogeneous CPU/NVMe offloading and careful scheduling [14]. LLM in a Flash exploits flash- based paging of parameters and KV to expand effective memory beyond DRAM [35]. Performance- model- guided systems such as LM- Offload choose placement and parallelism levels to meet latency and throughput targets under given resource caps [28]. These strategies dovetail with cloud- edge partitioning [22], [43], [70] and disaggregation [16], [23] to construct robust pipelines that sustain service quality across network jitters common in field deployments.  


### G. Edge RAG and Knowledge Caching  


Grounding responses with local knowledge is vital when connectivity is intermittent and privacy budgets are strict. EdgeRAG proposes online indexing adapted to edge devices, ensuring retrieval freshness and manageable memory [33]. RAGCache focuses on caching knowledge chunks and their usage signals to accelerate repeated queries while controlling staleness [18]. Hardware- algorithm co- design via computing- in- memory (RoCR) highlights that RAG primitives- embedding, top- \(k\) , and fusion- can run robustly close to sensors under tight energy budgets [78]. Collaborative serving (PETALS) supplies a complementary dimension where peers cache shards of knowledge or model states [26]. We use this body of work to motivate treating local retrieval and cross- device cooperation as primary design axes rather than afterthoughts.  


### H. Multimodal Signal-to-Language at the Edge  


Edge deployments often begin with continuous signals whose semantics are not directly textual. Instruction- tuned vision- language and audio- language models bridge that gap by mapping non- text inputs into prompts an LLM can reason over. LLaVA establishes a practical blueprint for visual instruction tuning, which is directly implementable on embedded GPUs and edge accelerators with appropriate compression [57]. AudioGPT extends this mapping to speech, music, and generic audio, highlighting a modular path to couple signal encoders with language backbones [39].  


In Wi- Fi and RFID sensing, a long line of research shows how to extract robust semantics from volatile channels. Correlation- based subcarrier selection resists co- channel interference for human activity recognition [29]. Phase- component analysis further mitigates interference to stabilize activity recognition in realistic environments, a core prerequisite for any reliable edge narrative over raw CSI [13]. Gesture recognition with attentional encoders demonstrates that commodity Wi- Fi devices can support fine- grained interaction patterns [25]. At the multimodal boundary, combining Wi- Fi and vision yields unobtrusive emotion recognition through gestures and facial expressions, suggesting that the best edge descriptors mix robust RF cues with lightweight vision [34]. Health- oriented sensing evolves this theme: in- area respiratory monitoring reframes Wi- Fi sensing for targeted healthcare [9], and Wi- Pulmo shows that pulmonary function can be captured  


without mouth- clinging, a compelling case for keeping processing close to the patient [32]. In parallel, open- set gesture recognition (WiOpen) emphasizes uncertainty modeling, crucial for production- grade edge inferences that must abstain or escalate on unfamiliar patterns [45]. For affective computing in the wild, label noise suppression (ReSup) addresses the noisy supervision that pervades edge datasets [21]. RFID- based writing and identity sensing (RF- Eye) broaden the RF modality palette and nudge systems to support multiple sensor backbones in the same edge node [48].  


These sensing results supply both application drivers and data characteristics for edge LLM systems: non- stationary distributions, abrupt interference, privacy sensitivity, and the need for online calibration. They motivate language- facing encoders at the edge—possibly trained or adapted via federated mechanisms—to emit compact, well- calibrated semantic tokens before any cross- boundary transmission.  


### I. Federated and Decentralized Optimization Under Edge Heterogeneity  


Personalization and data locality argue strongly for federated and decentralized learning to adapt encoders and lightweight adapters at the edge. Early resource- aware designs studied hierarchical aggregation and asynchronous updates to respect device variability and network budgets [51], [55], [56]. Subsequent work on semi- supervised regimes, progressive training, and architecture search refined how client encoders specialize under non- IID data [17], [60], [61], [75]. Decentralized aggregation and layer- wise strategies tackled partial participation and topology constraints [76], [77]. In heterogeneous edge networks, experience- driven model migration connects policy learning with deployment dynamics to select which models move where and when [5]. Recent directions include adaptive local update with neural composition for acceleration in heterogeneous networks [63], decentralized efficiency via probabilistic communication [64], and communication- efficient decentralized federated graph learning on non- IID data [66]. For fine- tuning large models in the federated setting, FedQuad combines layer- wise LoRA deployment with activation quantization to reduce memory and communication [62]. Catastrophic forgetting in federated fine- tuning can be alleviated with adaptive transformer block expansion [68].  


MoE- specific and sampling- centric improvements complement this toolbox. Adaptive expert split mechanisms speed up mixture- of- experts inference by balancing routing complexity [65], while more robust token sampling such as Top- \(n\) pruning in logit space improves generation stability under noisy on- device conditions [67]. Pipeline- level optimizations for end- cloud collaboration reduce idle bubbles along the path and are directly applicable to our routing stage between semantic encoders and LLM backbones [69]. This federated perspective ties back to adapter serving [3], [52] and LoRA/QLoRA [20], [74], sketching an end- to- end story where small, personal deltas are trained locally and served efficiently with quantization [2], [4], [10].